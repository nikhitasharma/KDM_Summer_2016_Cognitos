no title spark : Cluster Computing with work set Matei Zaharia , Mosharaf Chowdhury , Michael J. Franklin , Scott Shenker , Ion Stoica University of California , Berkeley abstract mapreduce and its variant have be highly successful in implement large-scale data-intensive application on commodity cluster . however , most of these system be build around a acyclic data flow model that be not suitable for other popular application . this paper focus on one such class of application : those that reuse a work set of datum across multiple parallel operation . this include many iterative machine learn algorithm , as well as interactive data analysis tool . we propose a new framework call Spark that support these application while retain the scalability and fault tolerance of MapReduce . to achieve these goal , spark introduce a abstraction call resilient distribute dataset -lrb- rdd -rrb- . a rdd be a read-only collection of object partition across a set of machine that can be rebuild if a partition be lose . spark can outperform hadoop by 10x in iterative machine learn job , and can be use to interactively query a 39 gb dataset with sub-second response time . 1 introduction a new model of cluster computing have become widely popular , in which data-parallel computation be execute on cluster of unreliable machine by system that automatically provide locality-aware scheduling , fault tolerance , and load balancing . mapreduce -lsb- 11 -rsb- pioneer this model , while system like Dryad -lsb- 17 -rsb- and map-reduce-merge -lsb- 24 -rsb- generalize the type of datum flow support . these system achieve they scalability and fault tolerance by provide a programming model where the user create acyclic datum flow graph to pass input datum through a set of operator . this allow the underlying system to manage scheduling and to react to fault without user intervention . while this data flow programming model be useful for a large class of application , there be application that can not be express efficiently as acyclic datum flow . in this paper , we focus on one such class of application : those that reuse a work set of datum across multiple parallel operation . this include two use case where we have see Hadoop user report that MapReduce be deficient : • iterative job : many commonmachine learning algorithm apply a function repeatedly to the same dataset to optimize a parameter -lrb- e.g. , through gradient descent -rrb- . while each iteration can be express as a MapReduce/Dryad job , each job must reload the datum from disk , incur a significant performance penalty . • interactive analytic : Hadoop be often use to run ad-hoc exploratory query on large dataset , through SQL interface such as pig -lsb- 21 -rsb- and Hive -lsb- 1 -rsb- . ideally , a user would be able to load a dataset of interest into memory across a number of machine and query it repeatedly . however , with Hadoop , each query incur significant latency -lrb- ten of seconds -rrb- because it run as a separate MapReduce job and read datum from disk . this paper present a new cluster computing framework call Spark , which support application with work set while provide similar scalability and fault tolerance property to MapReduce . the main abstraction in Spark be that of a resilient dis - tribute dataset -lrb- rdd -rrb- , which represent a read-only collection of object partition across a set of machine that can be rebuild if a partition be lose . user can explicitly cache a rdd in memory across machine and reuse it in multiple mapreduce-like parallel operation . rdd achieve fault tolerance through a notion of lineage : if a partition of a rdd be lose , the RDD have enough information about how it be derive from other rdd to be able to rebuild just that partition . although rdd be not a general shared memory abstraction , they represent a sweet-spot between expressivity on the one hand and scalability and reliability on the other hand , and we have find they well-suited for a variety of application . spark be implement in Scala -lsb- 5 -rsb- , a statically type high-level programming language for the Java VM , and expose a functional programming interface similar to dryadlinq -lsb- 25 -rsb- . in addition , spark can be use interactively from a modify version of the Scala interpreter , which allow the user to define rdd , function , variable and class and use they in parallel operation on a cluster . we believe that spark be the first system to allow a efficient , general-purpose programming language to be use interactively to process large dataset on a cluster . although we implementation of Spark be still a proto - type , early experience with the system be encourage . we show that spark can outperform hadoop by 10x in iterative machine learn workload and can be use interactively to scan a 39 gb dataset with sub-second latency . this paper be organize as follow . section 2 describe spark 's programming model and rdd . section 3 show some example job . section 4 describe we implementation , include we integration into Scala and its interpreter . section 5 present early result . we survey related work in section 6 and end with a discussion in section 7 . 2 Programming Model to use Spark , developer write a driver program that implement the high-level control flow of they application and launch various operation in parallel . spark provide two main abstraction for parallel programming : resilient distribute dataset and parallel operation on these dataset -lrb- invoke by pass a function to apply on a dataset -rrb- . in addition , spark support two restricted type of shared variable that can be use in function run on the cluster , which we shall explain later . 2.1 Resilient distribute dataset -lrb- rdd -rrb- a resilient distribute dataset -lrb- rdd -rrb- be a read-only collection of object partition across a set of machine that can be rebuild if a partition be lose . the element of a rdd need not exist in physical storage ; instead , a handle to a rdd contain enough information to compute the rdd start from datum in reliable storage . this mean that rdd can always be reconstruct if node fail . in Spark , each rdd be represent by a Scala object . spark let programmer construct rdd in four way : • from a file in a shared file system , such as the Hadoop distribute File System -lrb- hdfs -rrb- . • by `` parallelizing '' a Scala collection -lrb- e.g. , a array -rrb- in the driver program , which mean divide it into a number of slice that will be send to multiple node . • by transform a exist rdd . a dataset with element of typea can be transform into a dataset with element of type b use a operation call flatMap , which pass each element through a user-provided function of type a ⇒ list -lsb- b -rsb- .1 other transformation can be express use flatmap , include map -lrb- pass element through a function of type a ⇒ b -rrb- and filter -lrb- pick element match a predicate -rrb- . • by change the persistence of a exist rdd . by default , rdd be lazy and ephemeral . that be , partition of a dataset be materialize on demand when they be use in a parallel operation -lrb- e.g. , by pass a block of a file through a map function -rrb- , and be discard from memory after use .2 however , a user can alter the persistence of a rdd through two action : - the cache action leave the dataset lazy , but hint that it should be keep in memory after the first time it be compute , because it will be reuse . 1flatmap have the same semantics as the map in MapReduce , but map be usually use to refer to a one-to-one function of typea ⇒ b in Scala . 2this be how `` distribute collection '' function in dryadlinq . - the save action evaluate the dataset and write it to a distribute filesystem such as hdfs . the save version be use in future operation on it . we note that we cache action be only a hint : if there be not enough memory in the cluster to cache all partition of a dataset , Spark will recompute they when they be use . we choose this design so that spark program keep work -lrb- at reduce performance -rrb- if node fail or if a dataset be too big . this idea be loosely analogous to virtual memory . we also plan to extend spark to support other level of persistence -lrb- e.g. , in-memory replication across multiple node -rrb- . we goal be to let user trade off between the cost of store a rdd , the speed of access it , the probability of lose part of it , and the cost of recompute it . 2.2 parallel Operations several parallel operation can be perform on rdd : • reduce : combine dataset element use a associative function to produce a result at the driver program . • collect : send all element of the dataset to the driver program . for example , a easy way to update a array in parallel be to parallelize , map and collect the array . • foreach : pass each element through a user provide function . this be only do for the side effect of the function -lrb- which might be to copy datum to another system or to update a shared variable as explain below -rrb- . we note that spark do not currently support a group reduce operation as in MapReduce ; reduce result be only collect at one process -lrb- the driver -rrb- .3 we plan to support group reduction in the future use a `` shuffle '' transformation on distribute dataset , as describe in section 7 . however , even use a single reducer be enough to express a variety of useful algorithm . for example , a recent paper on MapReduce for machine learning on multicore system -lsb- 10 -rsb- implement ten learn algorithm without support parallel reduction . 2.3 share variable programmer invoke operation like map , filter and reduce by pass closure -lrb- function -rrb- to spark . as be typical in functional programming , these closure can refer to variable in the scope where they be create . normally , when Spark run a closure on a worker node , these variable be copy to the worker . however , spark also let programmer create two restricted type of shared variable to support two simple but common usage pattern : • Broadcast variable : if a large read-only piece of datum -lrb- e.g. , a lookup table -rrb- be use in multiple parallel operation , it be preferable to distribute it to the worker only once instead of packaging it with every closure . spark let the programmer create a `` broadcast vari - 3local reduction be first perform at each node , however . able '' object that wrap the value and ensure that it be only copy to each worker once . • accumulator : these be variable that worker can only `` add '' to use a associative operation , and that only the driver can read . they can be use to implement counter as in MapReduce and to provide a more imperative syntax for parallel sum . accumulator can be define for any type that have a `` add '' operation and a `` zero '' value . due to they `` add-only '' semantics , they be easy to make fault-tolerant . 3 example we now show some sample spark program . note that we omit variable type because Scala support type inference . 3.1 text search suppose that we wish to count the line contain error in a large log file store in hdfs . this can be implement by start with a file dataset object as follow : val file = spark.textfile -lrb- `` hdf : / / ... '' -rrb- val err = file.filter -lrb- _ . contain -lrb- `` error '' -rrb- -rrb- val one = errs.map -lrb- _ = > 1 -rrb- val count = ones.reduce -lrb- _ + _ -rrb- we first create a distribute dataset call file that represent the hdfs file as a collection of line . we transform this dataset to create the set of line contain `` error '' -lrb- err -rrb- , and then map each line to a 1 and add up these one use reduce . the argument to filter , map and reduce be Scala syntax for function literal . note that err and one be lazy rdd that be never materialize . instead , when reduce be call , each worker node scan input block in a streaming manner to evaluate one , add these to perform a local reduce , and send its local count to the driver . when use with lazy dataset in this manner , spark closely emulate MapReduce . where Spark differ from other framework be that it can make some of the intermediate dataset persist across operation . for example , if want to reuse the err dataset , we could create a cached rdd from it as follow : val cachederr = errs.cache -lrb- -rrb- we would now be able to invoke parallel operation on cachederr or on dataset derive from it as usual , but node would cache partition of cachederr in memory after the first time they compute they , greatly speed up subsequent operation on it . 3.2 Logistic regression the follow program implement logistic regression -lsb- 3 -rsb- , a iterative classification algorithm that attempt to find a hyperplane w that best separate two set of point . the algorithm perform gradient descent : it start w at a random value , and on each iteration , it sum a function of w over the datum to move w in a direction that improve it . it thus benefit greatly from cache the datum in memory across iteration . we do not explain logistic regression in detail , but we use it to show a few new spark feature . / / read point from a text file and cache they val point = spark.textfile -lrb- ... -rrb- . map -lrb- parsepoint -rrb- . cache -lrb- -rrb- / / initialize w to random d-dimensional vector var w = vector.random -lrb- d -rrb- / / Run multiple iteration to update w for -lrb- i < - 1 to iteration -rrb- -lcb- val grad = spark.accumulator -lrb- new Vector -lrb- D -rrb- -rrb- for -lrb- p < - point -rrb- -lcb- / / run in parallel val s = -lrb- 1 / -lrb- 1 + exp -lrb- - p.y * -lrb- w dot p.x -rrb- -rrb- -rrb- -1 -rrb- * p.y grad + = s * p.x -rcb- w - = grad.value -rcb- first , although we create a rdd call point , we process it by run a for loop over it . the for keyword in Scala be syntactic sugar for invoke the foreach method of a collection with the loop body as a closure . that be , the code for -lrb- p < - point -rrb- -lcb- body -rcb- be equivalent to points.foreach -lrb- p = > -lcb- body -rcb- -rrb- . therefore , we be invoke Spark 's parallel foreach operation . second , to sum up the gradient , we use a accumulator variable call gradient -lrb- with a value of type v ector -rrb- . note that the loop add to gradient use a overloaded + = operator . the combination of accumulator and for syntax allow spark program to look much like imperative serial program . indeed , this example differ from a serial version of logistic regression in only three line . 3.3 alternate Least square we final example be a algorithm call alternate least square -lrb- al -rrb- . ALS be use for collaborative filter problem , such as predict user ' rating for movie that they have not see base on they movie rating history -lrb- as in the Netflix Challenge -rrb- . unlike we previous example , ALS be cpu-intensive rather than data-intensive . we briefly sketch ALS and refer the reader to -lsb- 27 -rsb- for detail . suppose that we want to predict the rating of u user form movie , and that we have a partially fill matrix r contain the known rating for some user-movie pair . ALS model r as the product of two matrix m and u of dimension m × k and k × u respectively ; that be , each user and each movie have a k-dimensional `` feature vector '' describe its characteristic , and a user 's rating for a movie be the dot product of its feature vector and the movie 's . ALS solve for m and u use the known rating and then compute m × U to predict the unknown one . this be do use the follow iterative process : 1 . initializem to a random value . 2 . optimize U givenM to minimize error on R. 3 . optimizem give U to minimize error on R. 4 . repeat step 2 and 3 until convergence . ALS can be parallelize by update different user / movie on each node in step 2 and 3 . however , because all of the step use r , it be helpful to make r a broadcast variable so that it do not get re-sent to each node on each step . a spark implementation of ALS that do be show below . note that we parallelize the collection 0 until u -lrb- a scala range object -rrb- and collect it to update each array : val rb = spark.broadcast -lrb- r -rrb- for -lrb- i < - 1 to iteration -rrb- -lcb- u = spark.parallelize -lrb- 0 until u -rrb- . map -lrb- j = > updateuser -lrb- j , rb , m -rrb- -rrb- . collect -lrb- -rrb- m = spark.parallelize -lrb- 0 until m -rrb- . map -lrb- j = > updateuser -lrb- j , rb , U -rrb- -rrb- . collect -lrb- -rrb- -rcb- 4 implementation spark be build on top of mesos -lsb- 16 , 15 -rsb- , a `` cluster operating system '' that let multiple parallel application share a cluster in a fine-grained manner and provide a api for application to launch task on a cluster . this allow Spark to run alongside exist cluster computing framework , such as Mesos port of Hadoop andMPI , and share datum with they . in addition , build on Mesos greatly reduce the programming effort that have to go into Spark . the core of Spark be the implementation of resilient dis - tributed dataset . as a example , suppose that we define a cache dataset call cachederr represent error message in a log file , and that we count its element use map and reduce , as in section 3.1 : val file = spark.textfile -lrb- `` hdf : / / ... '' -rrb- val err = file.filter -lrb- _ . contain -lrb- `` error '' -rrb- -rrb- val cachederrs = errs.cache -lrb- -rrb- val one = cachederrs.map -lrb- _ = > 1 -rrb- val count = ones.reduce -lrb- _ + _ -rrb- these dataset will be store as a chain of object capture the lineage of each rdd , show in Figure 1 . each dataset object contain a pointer to its parent and information about how the parent be transform . internally , each rdd object implement the same sim - ple interface , which consist of three operation : • getpartition , which return a list of partition id . • getiterator -lrb- partition -rrb- , which iterate over a partition . • getpreferredlocation -lrb- partition -rrb- , which be use for task scheduling to achieve datum locality . when a parallel operation be invoke on a dataset , spark create a task to process each partition of the dataset and send these task to worker node . we try to send each HdfsTextFile path = hdf : / / ... file : FilteredDataset func = _ . contain -lrb- ... -rrb- CachedDataset err : cachederr : MappedDataset func = _ = > 1 one : Figure 1 : lineage chain for the distribute dataset object define in the example in section 4 . task to one of its preferred location use a technique call delay scheduling -lsb- 26 -rsb- . once launch on a worker , each task call getiterator to start read its partition . the different type of rdd differ only in how they implement the rdd interface . for example , for a Hdfs-TextFile , the partition be block id in hdfs , they prefer location be the block location , and getiterator open a stream to read a block . in a MappedDataset , the partition and prefer location be the same as for the parent , but the iterator apply the map function to element of the parent . finally , in a CachedDataset , the getiterator method look for a locally cache copy of a transform partition , and each partition 's preferred location start out equal to the parent 's preferred location , but get update after the partition be cache on some node to prefer reuse that node . this design make fault easy to handle : if a node fail , its partition be re-read from they parent dataset and eventually cache on other node . finally , ship task to worker require shipping closure to them-both the closure use to define a distribute dataset , and closure pass to operation such as reduce . to achieve this , we rely on the fact that Scala closure be Java object and can be serialize use Java serialization ; this be a feature of Scala that make it relatively straightforward to send a computation to another machine . Scala 's built-in closure implementation be not ideal , however , because we have find case where a closure object reference variable in the closure 's outer scope that be not actually use in its body . we have file a bug report about this , but in the meantime , we have solve the issue by perform a static analysis of closure class ' byte-code to detect these unused variable and set the corresponding field in the closure object to null . we omit the detail of this analysis due to lack of space . share Variables : the two type of shared variable in Spark , broadcast variable and accumulator , be implement use class with custom serialization format . when one create a broadcast variable b with a value v , v be save to a file in a shared file system . the serialized form of b be a path to this file . when b 's value be query on a worker node , spark first check whether v be in a local cache , and read it from the file system if it be not . we initially use hdfs to broadcast variable , but we be develop a more efficient streaming broadcast system . accumulator be implement use a different `` se - rialization trick . '' each accumulator be give a unique id when it be create . when the accumulator be save , its serialized form contain its ID and the `` zero '' value for its type . on the worker , a separate copy of the accumulator be create for each thread that run a task use thread-local variable , and be reset to zero when a task begin . after each task run , the worker send a message to the driver program contain the update it make to various accumulator . the driver apply update from each partition of each operation only once to prevent double-counting when task be re-executed due to failure . interpreter integration : due to lack of space , we only sketch how we have integrate Spark into the Scala interpreter . the Scala interpreter normally operate by compile a class for each line type by the user . this class include a singleton object that contain the variable or function on that line and run the line 's code in its constructor . for example , if the user type var x = 5 follow by println -lrb- x -rrb- , the interpreter define a class -lrb- say line1 -rrb- contain x and cause the second line to compile to println -lrb- line1.getinstance -lrb- -rrb- . x -rrb- . these class be load into the JVM to run each line . to make the interpreter work with spark , we make two change : 1 . we make the interpreter output the class it define to a shared filesystem , from which they can be load by the worker use a custom Java class loader . 2 . we change the generate code so that the singleton object for each line reference the singleton object for previous line directly , rather than go through the static getinstance method . this allow closure to capture the current state of the singleton they reference whenever they be serialize to be send to a worker . if we have not do this , then update to the singleton object -lrb- e.g. , a line setting x = 7 in the example above -rrb- would not propagate to the worker . 5 result although we implementation of Spark be still at a early stage , we relate the result of three experiment that show its promise as a cluster computing framework . Logistic regression : we compare the performance of the logistic regression job in section 3.2 to a implementation of logistic regression for Hadoop , use a 29 gb dataset on 20 `` m1.xlarge '' ec2 node with 4 core each . the result be show in figure 2 . with Hadoop , each iteration take 127 , because it run as a independent MapReduce job . with spark , the first iteration take 174 -lrb- likely due to use Scala instead of Java -rrb- , but subsequent 0 1000 2000 3000 4000 1 5 10 20 30 r un nus ng t im e -lrb- s -rrb- Number of Iterations Hadoop Spark figure 2 : logistic regression performance in Hadoop and Spark . iteration take only 6s , each because they reuse cache datum . this allow the job to run up to 10x faster . we have also try crash a node while the job be run . in the 10-iteration case , this slow the job down by 50s -lrb- 21 % -rrb- on average . the datum partition on the lose node be recompute and cache in parallel on other node , but the recovery time be rather high in the current experiment because we use a high hdfs block size -lrb- 128 mb -rrb- , so there be only 12 block per node and the recovery process could not utilize all core in the cluster . smaller block size would yield faster recovery time . alternate Least Squares : we have implement the alternate least square job in section 3.3 to measure the benefit of broadcast variable for iterative job that copy a shared dataset to multiple node . we find that without use broadcast variable , the time to resend the rating matrix r on each iteration dominate the job 's run time . furthermore , with a naı̈ve implementation of broadcast -lrb- use hdfs or nfs -rrb- , the broadcast time grow linearly with the number of node , limit the scalability of the job . we implement a application-level multicast system to mitigate this . however , even with fast broadcast , resend r on each iteration be costly . cache r in memory on the worker use a broadcast variable improved performance by 2.8 x in a experiment with 5000 movie and 15000 user on a 30-node ec2 cluster . interactive Spark : we use the spark interpreter to load a 39 gb dump of Wikipedia in memory across 15 `` m1.xlarge '' ec2 machine and query it interactively . the first time the dataset be query , it take roughly 35 seconds , comparable to run a Hadoop job on it . however , subsequent query take only 0.5 to 1 seconds , even if they scan all the datum . this provide a qualitatively different experience , comparable to work with local datum . 6 related work distribute Shared memory : Spark 's resilient distribute dataset can be view as a abstraction for distribute shared memory -lrb- DSM -rrb- , which have be study extensively -lsb- 20 -rsb- . rdd differ from DSM interface in two way . first , rdd provide a much more restricted programming model , but one that let dataset be rebuild efficiently if cluster node fail . while some DSM system achieve fault tolerance through checkpointing -lsb- 18 -rsb- , spark reconstruct lose partition of rdd use lineage information capture in the rdd object . this mean that only the lose partition need to be recompute , and that they can be recompute in parallel on different node , without require the program to revert to a checkpoint . in addition , there be no overhead if no node fail . second , rdd push computation to the datum as inmapreduce -lsb- 11 -rsb- , rather than let arbitrary node access a global address space . other system have also restrict the DSM program - ming model to improve performance , reliability and programmability . munin -lsb- 8 -rsb- let programmer annotate variable with the access pattern they will have so as to choose a optimal consistency protocol for they . Linda -lsb- 13 -rsb- provide a tuple space programming model that may be implement in a fault-tolerant fashion . Thor -lsb- 19 -rsb- provide a interface to persistent shared object . Cluster Computing Frameworks : Spark 's parallel operation fit into the MapReduce model -lsb- 11 -rsb- . however , they operate on rdd that can persist across operation . the need to extendmapreduce to support iterative job be also recognize by twister -lsb- 6 , 12 -rsb- , a MapReduce framework that allow long-lived map task to keep static datum in memory between job . however , Twister do not currently implement fault tolerance . spark 's abstraction of resilient distribute dataset be both fault-tolerant and more general than iterative MapReduce . a spark program can define multiple rdd and alternate between run operation on they , whereas a Twister program have only one map function and one reduce function . this also make spark useful for interactive data analysis , where a user can define several dataset and then query they . spark 's broadcast variable provide a similar facility to Hadoop 's distribute cache -lsb- 2 -rsb- , which can disseminate a file to all node run a particular job . however , broadcast variable can be reuse across parallel operation . language integration : Spark 's language integration be similar to that of dryadlinq -lsb- 25 -rsb- , which use . NET 's support for language integrate query to capture a expression tree define a query and run it on a cluster . unlike dryadlinq , spark allow rdd to persist in memory across parallel operation . in addition , spark enrich the language integration model by support shared variable -lrb- broadcast variable and accumulator -rrb- , implement use class with custom serialize form . we be inspire to use Scala for language integration by smr -lsb- 14 -rsb- , a Scala interface for Hadoop that use closure to define map and reduce task . we contribution over smr be share variable and a more robust implementation of closure serialization -lrb- describe in section 4 -rrb- . finally , ipython -lsb- 22 -rsb- be a python interpreter for scien - tist that let user launch computation on a cluster use a fault-tolerant task queue interface or low-level message pass interface . spark provide a similar interactive interface , but focus on data-intensive computation . lineage : capture lineage or provenance information for dataset have long be a research topic in the scientific compute a database field , for application such as explain result , allow they to be reproduce by other , and recompute datum if a bug be find in a work-flow step or if a dataset be lose . we refer the reader to -lsb- 7 -rsb- , -lsb- 23 -rsb- and -lsb- 9 -rsb- for survey of this work . spark provide a restricted parallel programming model where fine-grained lineage be inexpensive to capture , so that this information can be use to recompute lose dataset element . 7 discussion and future work spark provide three simple datum abstraction for programming cluster : resilient distribute dataset -lrb- rdd -rrb- , and two restricted type of shared variable : broadcast variable and accumulator . while these abstraction be limit , we have find that they be powerful enough to express several application that pose challenge for exist cluster computing framework , include iterative and interactive computation . furthermore , we believe that the core idea behind rdd , of a dataset handle that have enough information to -lrb- re -rrb- construct the dataset from datum available in reliable storage , may prove useful in develop other abstraction for programming cluster . in future work , we plan to focus on four area : 1 . formally characterize the property of rdd and spark 's other abstraction , and they suitability for various class of application and workload . 2 . enhance the rdd abstraction to allow programmer to trade between storage cost and re-construction cost . 3 . define new operation to transform rdd , include a `` shuffle '' operation that repartition a rdd by a give key . such a operation would allow we to implement group-by and join . 4 . provide higher-level interactive interface on top of the spark interpreter , such as sql and r -lsb- 4 -rsb- shell . 8 acknowledgement we thank Ali Ghodsi for he feedback on this paper . this research be support by California MICRO , califor-nia Discovery , the Natural Sciences and Engineering Research Council of Canada , as well as the follow berke-ley rad lab sponsor : Sun Microsystems , Google , mi-crosoft , Amazon , Cisco , Cloudera , eBay , Facebook , fu-jitsu , HP , Intel , NetApp , sap , VMware , and yahoo! . reference -lsb- 1 -rsb- Apache Hive . http://hadoop.apache.org/hive . -lsb- 2 -rsb- Hadoop Map/Reduce tutorial . http://hadoop.apache.org/ common/docs/r0 .20.0 / mapred tutorial.html . -lsb- 3 -rsb- Logistic regression - Wikipedia . http://en.wikipedia.org/wiki/logistic regression . -lsb- 4 -rsb- the r project for statistical computing . http://www.r-project.org . -lsb- 5 -rsb- Scala programming language . http://www.scala-lang.org . -lsb- 6 -rsb- Twister : iterative mapreduce . http://iterativemapreduce.org . -lsb- 7 -rsb- R. Bose and J. Frew . lineage retrieval for scientific datum processing : a survey . ACM Computing survey , 37:1 -28 , 2005 . -lsb- 8 -rsb- J. B. Carter , J. K. Bennett , and W. Zwaenepoel . implementation and performance of munin . in sosp ' 91 . ACM , 1991 . -lsb- 9 -rsb- J. Cheney , L. Chiticariu , and w.-c . Tan . Provenance in database : why , how , and where . foundation and trend in database , 1 -lrb- 4 -rrb- :379 -474 , 2009 . -lsb- 10 -rsb- C. T. Chu , S. K. Kim , Y. A. Lin , Y. Yu , G. R. Bradski , A. Y. Ng , and K. Olukotun . map-reduce for machine learning on multicore . in nip ' 06 , page 281-288 . MIT Press , 2006 . -lsb- 11 -rsb- J. Dean and S. Ghemawat . MapReduce : simplify datum processing on large cluster . Commun . ACM , 51 -lrb- 1 -rrb- :107 -113 , 2008 . -lsb- 12 -rsb- J. Ekanayake , S. Pallickara , and G. Fox . MapReduce for datum intensive scientific analysis . in escience ' 08 , page 277-284 , Washington , DC , USA , 2008 . IEEE Computer Society . -lsb- 13 -rsb- D. Gelernter . generative communication in linda . ACM Trans . program . Lang . Syst. , 7 -lrb- 1 -rrb- :80 -112 , 1985 . -lsb- 14 -rsb- D. Hall . a scalable language , and a scalable framework . http://www.scala-blogs.org/2008/09/scalable-language-and-scalable.html . -lsb- 15 -rsb- B. Hindman , A. Konwinski , M. Zaharia , A. Ghodsi , A. D. Joseph , R. H. Katz , S. Shenker , and i. Stoica . Mesos : A platform for fine-grained resource sharing in the datum center . Technical Report ucb/eecs -2010 -87 , EECS Department , University of California , Berkeley , May 2010 . -lsb- 16 -rsb- B. Hindman , A. Konwinski , M. Zaharia , and i. Stoica . a common substrate for cluster computing . InWorkshop on Hot Topics in Cloud Computing -lrb- HotCloud -rrb- 2009 , 2009 . -lsb- 17 -rsb- M. Isard , M. Budiu , Y. Yu , A. Birrell , and D. Fetterly . Dryad : distribute data-parallel program from sequential building block . in EuroSys 2007 , page 59-72 , 2007 . -lsb- 18 -rsb- A.-M . Kermarrec , G. Cabillic , A. Gefflaut , C. Morin , and i. puaut . a recoverable distribute share memory integrate coherence and recoverability . in ftcs ' 95 . IEEE Computer Society , 1995 . -lsb- 19 -rsb- B. Liskov , A. Adya , M. Castro , S. Ghemawat , R. Gruber , U. Maheshwari , A. C. Myers , M. Day , and L. Shrira . safe and efficient sharing of persistent object in thor . in sigmod ' 96 , page 318-329 . ACM , 1996 . -lsb- 20 -rsb- B. Nitzberg and V. Lo . distribute shared memory : a survey of issue and algorithm . computer , 24 -lrb- 8 -rrb- :52 -60 , aug 1991 . -lsb- 21 -rsb- C. Olston , B. Reed , U. Srivastava , R. Kumar , and a. Tomkins . pig latin : a not-so-foreign language for datum processing . in sigmod ' 08 . ACM , 2008 . -lsb- 22 -rsb- F. Pérez and B. E. Granger . IPython : a system for interactive scientific computing . Comput . Sci . Eng. , 9 -lrb- 3 -rrb- :21 -29 , May 2007 . -lsb- 23 -rsb- Y. L. Simmhan , B. Plale , and D. Gannon . a survey of datum provenance in e-science . SIGMOD Rec. , 34 -lrb- 3 -rrb- :31 -36 , 2005 . -lsb- 24 -rsb- h.-c . Yang , A. Dasdan , r.-l . Hsiao , and D. S. Parker . map-reduce-merge : simplify relational datum processing on large cluster . in sigmod ' 07 , page 1029-1040 . ACM , 2007 . -lsb- 25 -rsb- Y. Yu , M. Isard , D. Fetterly , M. Budiu , Ú . Erlingsson , P. K. Gunda , and J. Currey . DryadLINQ : a system for general-purpose distribute data-parallel computing use a high-level language . in osdi ' 08 , San Diego , CA , 2008 . -lsb- 26 -rsb- M. Zaharia , D. Borthakur , J. Sen Sarma , K. Elmeleegy , S. Shenker , and i. Stoica . delay scheduling : a simple technique for achieve locality and fairness in cluster scheduling . in EuroSys 2010 , April 2010 . -lsb- 27 -rsb- Y. Zhou , D. Wilkinson , R. Schreiber , and R. Pan . large-scale parallel collaborative filter for the Netflix prize . in aaim ' 08 , page 337-348 , Berlin , Heidelberg , 2008 . Springer-Verlag . 